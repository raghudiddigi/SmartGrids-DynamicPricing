import tensorflow as tf
import collections
import random
import numpy as np
import math
import copy

np.random.seed(7)
random.seed(7)

# This is the class which implements the Microgrid
class DQN_Agent:
# Initialise the Microgrid object and its variables 
    def __init__(self, name, state_size, max_battery, max_energy_generated, max_received, min_non_adl, max_non_adl, grid_price, total_iterations, current_iteration, lam, non_adl):
        self.name = name
        self.state_size = state_size
        self.max_battery = max_battery
        self.max_energy_generated = max_energy_generated
        self.max_received = max_received
        self.grid_price = grid_price
        self.action_size_pricing = (max_battery + max_energy_generated) * 6 + max_received + 1  # 6 for grid price to (grid price - 5) and 1 for the zeroth state.
        self.action_size_adl = 8
        self.total_iterations = total_iterations
        self.current_iteration = current_iteration
        self.memory = collections.deque(maxlen = 10000)
        self.gamma = 0.90    # discount rate
        self.epsilon = 0.8  # exploration rate
        self.epsilon_min = 0
        self.regularizer_loss = 50  # Loss for penalising the impossible actions
        self.regularizer_factor = 0.07
        self.pricing_model = self._build_model_pricing()
        self.adl_model = self._build_model_adl()
        if non_adl:
            self.non_adl = [2, 2.5, 3.5, 4]
        else:
            self.non_adl = [0, 0, 0, 0]
        self.prob_non_adl = [[0.4,0.3,0.2,0.1],[0.1,0.4,0.3,0.2],[0.1,0.3,0.4,0.2],[0.2,0.3,0.1,0.4]]
        self.adl_value = [[1, 2], [1, 3], [2, 4]]
        self.lam = lam
        self.adl_state = 7

    #
    #This is the function that returns the renewable power generated by the Microgrid at the particular time. It samples from a poisson
    # distribution. We get the lamda value for the poisson distribution by preprocessing a data set.
    def get_renewable(self,time):
        energy = np.random.poisson(lam=self.lam[time-1],size=1)
        energy = min([10, energy]) # clipping the value so that it can't exceede 8
        energy = int(math.floor(energy))
        return energy
    
    #
    #This is the function that returns the non adl demand for the given grid at the particular time instant.
    def get_non_adl_demand(self,time):
        demand = np.random.choice(self.non_adl,p=self.prob_non_adl[time-1])
        return int(demand)

    #
    #Custom Loss function for the neural network
    def custom_loss(self, y_true, y_pred):
        loss = tf.keras.backend.mean(tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred), axis=1)) # need to put axis
        return loss

    # This is Pricing model. This takes the state and ADL action chosen as input and outputs the expected rewards for all pricing decisons (both the price
    # chosen by the microgrid to sell at and the amount of electricty to be traded)

    def _build_model_pricing(self):

        # nd, d, adl_action , t , gp
        model = tf.keras.models.Sequential()
        model.add(tf.keras.layers.Dense(32,input_shape=(5,), activation = tf.keras.activations.relu,kernel_regularizer=tf.keras.regularizers.l2(2e-4)))
        model.add(tf.keras.layers.Dense(32, activation = tf.keras.activations.relu,kernel_regularizer=tf.keras.regularizers.l2(2e-4)))
        model.add(tf.keras.layers.Dense(self.action_size_pricing))
        optim = tf.keras.optimizers.Adam(lr = 0.0001)
        model.compile(loss = self.custom_loss, optimizer = optim)
        return model

    #
    # This is the ADL model for the Microgrid. This takes the state as input and outputs the expected rewards for all ADL action  

    def _build_model_adl(self):

    	# nd, d, adl_state, t, gp
        model = tf.keras.models.Sequential()
        model.add(tf.keras.layers.Dense(16,input_shape=(5,), activation = tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.l2(2e-4)))
        model.add(tf.keras.layers.Dense(16, activation = tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.l2(2e-4)))
        model.add(tf.keras.layers.Dense(self.action_size_adl))
        optim = tf.keras.optimizers.Adam(lr = 0.0001)
        model.compile(loss = self.custom_loss, optimizer = optim)
        return model

    #
    def summary(self):
        self.pricing_model.summary()
        print('-------------------')
        self.adl_model.summary()
        print('-------------------')

    # Gives the range of allowed pricing actions given the state of the microgrid. Her we impose the design constraints 
    # to get the range of possible actions. The design constraints give us the range of electricty values that can be traded. Negative values means that a grid is buying 
    # while positve values mean that a grid is selling. This range is converted to represent range of pricing action indexes .Each index represents an action pair of the amount of  
    # electricty to be traded and the price to be traded at. The function returns a tuple which has the lower bound index and the upper bound index on actions that can be taken.
    # Whenever a Microgrid takes a decision to buy electricty we associate no price with it. When it decides to sell electricty
    # there are 6 possible prices that can be associated with it from GP to GP-5. So negative values(buying) have a single pricing action index associated with them
    # while each positve value of energy to be traded(selling) has pricing six pricing action indexes associated with them.

    def pricing_convert_constraint_values_to_allowed_action_indices(self, state):

        # State contains nd, d, adl, t, gp
        nd = state[0]
        d = state[1]
        adl = self.adl_convert_allowed_indices_to_values(state[2])
        lower_bound = max(-1*self.max_received, nd - self.max_battery - adl)
        upper_bound = nd + d - adl

        if (lower_bound <= 0):
            lower_bound_index = lower_bound + self.max_received
        else:
            lower_bound_index = self.max_received + (lower_bound - 1) * 6 + 1  # In order to account for the zero state

        if (upper_bound <= 0):
            upper_bound_index = upper_bound + self.max_received
        else:
            upper_bound_index = self.max_received + upper_bound * 6

        return lower_bound_index, upper_bound_index

    # convert pricing action index to the respective price quoted and energy to be traded

    def pricing_convert_allowed_indices_to_values(self, action):
        # return price and ut 
        if (action <= self.max_received):
            return([0, action -self.max_received])
        else :
            action = action - self.max_received - 1 
            return([action % 6 + self.grid_price - 5 , action // 6 + 1])
        
   # Takes a pricing action based on the available pricing action indexes using an epsilon greedy policy(either choose a random action with epsilon proabilty
    #  or choose an action that maximises your expected reward) 

    def pricing_action(self, state):

        lower_bound_index, upper_bound_index = self.pricing_convert_constraint_values_to_allowed_action_indices(state)
        check = random.uniform(0, 1)
        state = np.array([state])
        possible_actions = []
        for i in range(lower_bound_index, upper_bound_index + 1):
            if i <= self.max_received:
                possible_actions.append(i)
            elif ((i - self.max_received) % 6) != 7:
                possible_actions.append(i)
        if (check < self.epsilon):
            return np.random.choice(possible_actions)
        else:
            output = self.pricing_model.predict(state)
            index = 0
            max_ = - 50000000
            for i in possible_actions:
                if max_ < output[0][i]:
                    max_ = output[0][i]
                    index = i

            return index
    # given the state and the possible pricing actions returns the maximum expected reward over all the possible actions as predicted by the Pricing neural network

    def price_max_Q(self,state,li,ui):
        lower_bound_index, upper_bound_index = li,ui
        state = np.array([state])
        possible_actions = []
        for i in range(lower_bound_index, upper_bound_index + 1):
            if i <= self.max_received:
                possible_actions.append(i)
            elif ((i - self.max_received) % 6) != 7:
                possible_actions.append(i)
        output = self.pricing_model.predict(state)
        max_ = -50000000
        for i in possible_actions:
            if max_ < output[0][i]:
                max_ = output[0][i]
        return max_


    # Returns an array of possible ADL actions. Given the ADL state returns an array of all the possible ADL actions taht can be taken
    # The ADL states are represented using a binary encoding. Each ADL state is an integer value between 0-7 where the integer's binary 
    # form has a meaning associated with it. Take an ADL state of 6. 6 in binary is 110. Here ith index represents if the ith ADL demand has been 
    # scheduled or not. 1 means that it has not been scheduled till now and 0 means that it has been completed. Similarly each ADL action 
    # is also a integer between 0-7 with the binary form associated with it has a meaning. Take an ADL action of 6. 6 in binary is 110. Here ith
    # index means that the agent has decided to schedule the ith demand. So 110 means that the agent has decided to schedule the 3rd and 2nd ADL demand.   
    def adl_give_possible_actions(self, state):

        possible_actions = []
        possible_actions.append(0)
        for i in range(3):
            if (state & 2**i):
                temp = copy.deepcopy(possible_actions)
                for j in range(len(temp)):
                    temp[j] += 2**i

                possible_actions.extend(temp)
        return possible_actions

        # given an adl action taken returns the amount of energy that is needed to satisfy that ADL action 

    def adl_convert_allowed_indices_to_values(self, action):
        adl = 0
        temp = action
        for j in range(3):
            if (temp % 2 == 1):
                adl += self.adl_value[j][0]
            temp = temp//2
        return adl
    
    # given a state returns the adl action using an epsilon greedy policy (take a random ADL actions or an action that maximises the predicted
    # expected reward ) 
    def adl_action(self, state):

        possible_actions = sorted(self.adl_give_possible_actions(state[2]))
        check = random.uniform(0, 1)
        if (check < self.epsilon):
            return np.random.choice(possible_actions)
        else:
            index = 0
            max_ = - 50000000
            output = self.adl_model.predict(np.asarray([state]))
            for i in possible_actions:
                if max_ < output[0][i]:
                    max_ = output[0][i]
                    index = i

            return index  

    # Returns the max expected rewards for all the possible ADL actions given the state

    def adl_max_Q(self, state):

        possible_actions = sorted(self.adl_give_possible_actions(state[2]))
        index = 0
        max_ = - 50000000
        output = self.adl_model.predict(np.asarray([state]))
        for i in possible_actions:
            if max_ < output[0][i]:
                max_ = output[0][i]
                index = i

        return max_ 

    # Given the ADL action updates the ADL state. It returns the penalty if there is one for not satisfying the ADL demand.

    def update_adl(self,adl_action, time):
        self.adl_state = self.adl_state & (~adl_action)
        penalty = 0
        if time==2 and (self.adl_state & 1):
            self.adl_state = self.adl_state & (~1)
            penalty = 1
        elif time==3 and (self.adl_state & 2):
            self.adl_state = self.adl_state & (~2)
            penalty = 1
        elif time==4 and (self.adl_state & 4):
            penalty = 2
            self.adl_state = 7
        if time==4:
            self.adl_state = 7
        new_adl_state = copy.deepcopy(self.adl_state)
        return penalty, new_adl_state             
# Stores the action into the repaly memory buffer
    def remember(self, state_adl, state_price, action_adl, action_price, reward, next_state_adl, next_state_price):
        self.memory.append((state_adl, state_price, action_adl, action_price, reward, next_state_adl, next_state_price))

    def load_model(self,path_pricing, path_adl):
        self.pricing_model.load_weights(path_pricing)
        self.adl_model.load_weights(path_adl)
    # Samples a mini batch from the replay buffer and fits the neural networks on the mini batch 

    def replay(self, batch_size):

        minibatch = random.sample(self.memory, batch_size)
        states_batch_pricing, targets_batch_pricing, regularizer_batch = [], [], []
        states_batch_adl, targets_batch_adl = [], []
        y_true = []

        for state_adl, state_price, action_adl,action_price, reward, next_state_adl, next_state_price in minibatch:
            
            dl_state, dl_next_state = np.array([state_price]),np.array([next_state_price])
            lower_bound_index_ns, upper_bound_index_ns = self.pricing_convert_constraint_values_to_allowed_action_indices(next_state_price)
            target_price = reward / 180.0 + self.gamma * self.price_max_Q(next_state_price,lower_bound_index_ns,upper_bound_index_ns)
            target_array_price = self.pricing_model.predict(dl_state)
            target_array_price[0][action_price] = target_price

            dl_adl_state, dl_adl_next_state = np.array([state_adl]), np.array([next_state_adl])
            target_adl = reward / 180.0 + self.gamma * self.adl_max_Q(next_state_adl)
            target_array_adl = self.adl_model.predict(dl_adl_state)
            target_array_adl[0][action_adl] = target_adl
            
            states_batch_pricing.append(state_price)
            targets_batch_pricing.append(target_array_price[0])
            states_batch_adl.append(state_adl)
            targets_batch_adl.append(target_array_adl[0])
    

        history_pricing = self.pricing_model.fit(np.array(states_batch_pricing), np.array(targets_batch_pricing), epochs=1, verbose=0)
        history_adl = self.adl_model.fit(np.array(states_batch_adl), np.array(targets_batch_adl), epochs=1, verbose=0)
        loss_pricing = history_pricing.history['loss'][0]
        loss_adl = history_adl.history['loss'][0]
        self.epsilon = max(self.epsilon_min, (0.8 - self.current_iteration/self.total_iterations))
        
        self.current_iteration += 4

        return loss_adl,loss_pricing

    def save_model(self):
        self.pricing_model.save_weights('./saved/'  + self.name + '_pricing_model_adl_pricing' + '.h5')
        self.adl_model.save_weights('./saved/'  + self.name + '_adl_model_adl_pricing' + '.h5')

# This is similar as DQN_Agent but the agent can only quote a single price for selling 
class DQN_Agent_Price_Constant:

    def __init__(self, name, state_size, max_battery, max_energy_generated, max_received, min_non_adl, max_non_adl, grid_price, total_iterations, current_iteration, lam, non_adl):
        self.name = name
        self.state_size = state_size
        self.max_battery = max_battery
        self.max_energy_generated = max_energy_generated
        self.max_received = max_received
        self.grid_price = grid_price
        self.action_size_pricing = (max_battery + max_energy_generated) * 6 + max_received + 1  # 6 for grid price to (grid price - 5) and 1 for the zeroth state.
        self.action_size_adl = 8
        self.total_iterations = total_iterations
        self.current_iteration = current_iteration
        self.memory = collections.deque(maxlen = 10000)
        self.gamma = 0.90    # discount rate
        self.epsilon = 0.8  # exploration rate
        self.epsilon_min = 0
        self.regularizer_loss = 50  # Loss for penalising the impossible actions
        self.regularizer_factor = 0.07
        self.pricing_model = self._build_model_pricing()
        self.adl_model = self._build_model_adl()
        if non_adl:
            self.non_adl = [2, 2.5, 3.5, 4]
        else:
            self.non_adl = [0, 0, 0, 0]
        self.prob_non_adl = [[0.4,0.3,0.2,0.1],[0.1,0.4,0.3,0.2],[0.1,0.3,0.4,0.2],[0.2,0.3,0.1,0.4]]
        self.adl_value = [[1, 2], [1, 3], [2, 4]]
        self.lam = lam
        self.adl_state = 7

    #
    def get_renewable(self,time):
        energy = np.random.poisson(lam=self.lam[time-1],size=1)
        energy = min([10, energy]) # clipping the value so that it can't exceede 8
        energy = int(math.floor(energy))
        return energy
    
    #
    def get_non_adl_demand(self,time):
        demand = np.random.choice(self.non_adl,p=self.prob_non_adl[time-1])
        return int(demand)

    #
    def custom_loss(self, y_true, y_pred):
        loss = tf.keras.backend.mean(tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred), axis=1)) # need to put axis
        return loss

    #
    def _build_model_pricing(self):

        # nd, d, adl_action , t , gp
        model = tf.keras.models.Sequential()
        model.add(tf.keras.layers.Dense(32,input_shape=(5,), activation = tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.l2(2e-4)))
        model.add(tf.keras.layers.Dense(32, activation = tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.l2(2e-4)))
        model.add(tf.keras.layers.Dense(self.action_size_pricing))
        optim = tf.keras.optimizers.Adam(lr = 0.0001)
        model.compile(loss = self.custom_loss, optimizer = optim)
        return model

    #
    def _build_model_adl(self):

        # nd, d, adl_state, t, gp
        model = tf.keras.models.Sequential()
        model.add(tf.keras.layers.Dense(16,input_shape=(5,), activation = tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.l2(2e-4)))
        model.add(tf.keras.layers.Dense(16, activation = tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.l2(2e-4)))
        model.add(tf.keras.layers.Dense(self.action_size_adl))
        optim = tf.keras.optimizers.Adam(lr = 0.0001)
        model.compile(loss = self.custom_loss, optimizer = optim)
        return model

    #
    def summary(self):
        self.pricing_model.summary()
        print('-------------------')
        self.adl_model.summary()
        print('-------------------')

    #
    def pricing_convert_constraint_values_to_allowed_action_indices(self, state):

        # State contains nd, d, adl, t, gp
        nd = state[0]
        d = state[1]
        adl = self.adl_convert_allowed_indices_to_values(state[2])
        lower_bound = max(-1*self.max_received, nd - self.max_battery - adl)
        upper_bound = nd + d - adl

        if (lower_bound <= 0):
            lower_bound_index = lower_bound + self.max_received
        else:
            lower_bound_index = self.max_received + (lower_bound - 1) * 6 + 1  # In order to account for the zero state

        if (upper_bound <= 0):
            upper_bound_index = upper_bound + self.max_received
        else:
            upper_bound_index = self.max_received + upper_bound * 6

        return lower_bound_index, upper_bound_index

    #
    def pricing_convert_allowed_indices_to_values(self, action):
        # return price and ut 
        if (action <= self.max_received):
            return([0, action -self.max_received])
        else :
            action = action - self.max_received - 1 
            return([action % 6 + self.grid_price - 5 ,action // 6 + 1])

    #
    def pricing_action(self, state):

        lower_bound_index, upper_bound_index = self.pricing_convert_constraint_values_to_allowed_action_indices(state)
        check = random.uniform(0, 1)
        state = np.array([state])
        possible_actions = []
        for i in range(lower_bound_index, upper_bound_index + 1):
            if i <= self.max_received:
                possible_actions.append(i)
            elif ((i - self.max_received) % 6) == 0:
                possible_actions.append(i)
        if (check < self.epsilon):
            return np.random.choice(possible_actions)
        else:
            output = self.pricing_model.predict(state)
            index = 0
            max_ = - 500000
            for i in possible_actions:
                if max_ < output[0][i]:
                    max_ = output[0][i]
                    index = i

            return index 

    def price_max_Q(self,state,li,ui):
        lower_bound_index, upper_bound_index = li,ui
        state = np.array([state])
        possible_actions = []
        for i in range(lower_bound_index, upper_bound_index + 1):
            if i <= self.max_received:
                possible_actions.append(i)
            elif ((i - self.max_received) % 6) == 0:
                possible_actions.append(i)
        output = self.pricing_model.predict(state)
        max_ = -500000
        for i in possible_actions:
            if max_ < output[0][i]:
                max_ = output[0][i]
        return max_
    
    # 
    def adl_give_possible_actions(self, state):

        possible_actions = []
        possible_actions.append(0)
        for i in range(3):
            if (state & 2**i):
                temp = copy.deepcopy(possible_actions)
                for j in range(len(temp)):
                    temp[j] += 2**i

                possible_actions.extend(temp)
        return possible_actions

    # 
    def adl_convert_allowed_indices_to_values(self, action):
        adl = 0
        temp = action
        for j in range(3):
            if (temp % 2 == 1):
                adl += self.adl_value[j][0]
            temp = temp//2
        return adl
    
    #
    def adl_action(self, state):

        possible_actions = sorted(self.adl_give_possible_actions(state[2]))
        check = random.uniform(0, 1)
        if (check < self.epsilon):
            return np.random.choice(possible_actions)
        else:
            index = 0
            max_ = -500000
            output = self.adl_model.predict(np.asarray([state]))
            for i in possible_actions:
                if max_ < output[0][i]:
                    max_ = output[0][i]
                    index = i

            return index  

    # Give the action with max Q value 
    #
    def adl_max_Q(self, state):

        possible_actions = sorted(self.adl_give_possible_actions(state[2]))
        index = 0
        max_ = -500000
        output = self.adl_model.predict(np.asarray([state]))
        for i in possible_actions:
            if max_ < output[0][i]:
                max_ = output[0][i]
                index = i

        return max_ 

    #
    def update_adl(self,adl_action, time):
        self.adl_state = self.adl_state & (~adl_action)
        penalty = 0
        if time==2 and (self.adl_state & 1):
            self.adl_state = self.adl_state & (~1)
            penalty = 1
        elif time==3 and (self.adl_state & 2):
            self.adl_state = self.adl_state & (~2)
            penalty = 1
        elif time==4 and (self.adl_state & 4):
            penalty = 2
            self.adl_state = 7
        if time==4:
            self.adl_state = 7
        new_adl_state = copy.deepcopy(self.adl_state)
        return penalty, new_adl_state             

    def remember(self, state_adl, state_price, action_adl, action_price, reward, next_state_adl, next_state_price):
        self.memory.append((state_adl, state_price, action_adl, action_price, reward, next_state_adl, next_state_price))

    def load_model(self,path_pricing, path_adl):
        self.pricing_model.load_weights(path_pricing)
        self.adl_model.load_weights(path_adl)

    def replay(self, batch_size):

        minibatch = random.sample(self.memory, batch_size)
        states_batch_pricing, targets_batch_pricing, regularizer_batch = [], [], []
        states_batch_adl, targets_batch_adl = [], []
        y_true = []

        for state_adl, state_price, action_adl,action_price, reward, next_state_adl, next_state_price in minibatch:
            
            dl_state, dl_next_state = np.array([state_price]),np.array([next_state_price])
            lower_bound_index_ns, upper_bound_index_ns = self.pricing_convert_constraint_values_to_allowed_action_indices(next_state_price)
            target_price = reward / 180.0 + self.gamma * self.price_max_Q(next_state_price,lower_bound_index_ns,upper_bound_index_ns) # Normalized Rewards
            target_array_price = self.pricing_model.predict(dl_state)
            target_array_price[0][action_price] = target_price

            dl_adl_state, dl_adl_next_state = np.array([state_adl]), np.array([next_state_adl])
            target_adl = reward / 180.0 + self.gamma * self.adl_max_Q(next_state_adl)
            target_array_adl = self.adl_model.predict(dl_adl_state)
            target_array_adl[0][action_adl] = target_adl
            
            states_batch_pricing.append(state_price)
            targets_batch_pricing.append(target_array_price[0])
            states_batch_adl.append(state_adl)
            targets_batch_adl.append(target_array_adl[0])
    

        history_pricing = self.pricing_model.fit(np.array(states_batch_pricing), np.array(targets_batch_pricing), epochs=1, verbose=0)
        history_adl = self.adl_model.fit(np.array(states_batch_adl), np.array(targets_batch_adl), epochs=1, verbose=0)
        loss_pricing = history_pricing.history['loss'][0]
        loss_adl = history_adl.history['loss'][0]
        self.epsilon = max(self.epsilon_min, (0.8 - self.current_iteration/self.total_iterations))
        
        self.current_iteration += 4

        return loss_adl,loss_pricing

    def save_model(self):
        self.pricing_model.save_weights('./saved/'  + self.name + '_pricing_model_adl_pricing' + '.h5')
        self.adl_model.save_weights('./saved/'  + self.name + '_adl_model_adl_pricing' + '.h5')



